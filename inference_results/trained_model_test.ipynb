{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4605e78b",
   "metadata": {},
   "source": [
    "# SmolLM2-1.7B Training Results Comparison\n",
    "\n",
    "This notebook compares the original SmolLM2-1.7B model with our trained final_model.\n",
    "\n",
    "## Training Summary\n",
    "- **Dataset**: Cosmopedia-v2 (1B tokens)\n",
    "- **Steps Trained**: 30,011 steps\n",
    "- **Final Loss**: 3.7547\n",
    "- **Training Time**: ~13 hours\n",
    "\n",
    "## Test Configuration\n",
    "- **Generation Parameters**: temperature=0.7, top_p=0.9, max_new_tokens=30\n",
    "- **Prompts**: Same as used during training validation\n",
    "- **Memory Strategy**: Load models sequentially to avoid OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "859cfe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA L4\n",
      "Memory: 23.7GB\n",
      "GPU 1: NVIDIA L4\n",
      "Memory: 23.7GB\n",
      "GPU 2: NVIDIA L4\n",
      "Memory: 23.7GB\n",
      "GPU 3: NVIDIA L4\n",
      "Memory: 23.7GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "# Check available GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f}GB\")\n",
    "else:\n",
    "    print(\"CUDA not available - using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e41fa5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_MODEL_NAME = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "CHECKPOINT_PATH = \"/home/ubuntu/bigdata/Training/Day4/cosmopedia-v2-1B/smollm-1.7B-cosmo-1B-production/final_model\"\n",
    "\n",
    "# Generation parameters (same as training)\n",
    "GENERATION_CONFIG = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_new_tokens\": 30,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": None  # Will be set when tokenizer is loaded\n",
    "}\n",
    "\n",
    "# Test prompts (same as used during training)\n",
    "TEST_PROMPTS = [\n",
    "    \"The weather today is very\",\n",
    "    \"Machine learning is a field of\",\n",
    "    \"The capital of France is\",\n",
    "    \"In the year 2024, technology\",\n",
    "    \"Artificial intelligence can help\",\n",
    "    \"The most important thing in life is\",\n",
    "    \"Scientists have recently discovered\",\n",
    "    \"The future of renewable energy\",\n",
    "    \"Education is essential because\",\n",
    "    \"Climate change represents\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1da7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_memory():\n",
    "    \"\"\"Aggressive memory cleanup\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def load_model_and_tokenizer(model_path: str, is_checkpoint: bool = False) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"Load model and tokenizer with memory optimization\"\"\"\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    if is_checkpoint:\n",
    "        # For checkpoint, tokenizer is in the same directory\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    else:\n",
    "        # For base model, load from HuggingFace\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model with memory optimization\n",
    "    if is_checkpoint:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    else:\n",
    "        # For Base Model, load from Config\n",
    "        config = AutoConfig.from_pretrained(model_path)\n",
    "        # config.use_cache = False  # Essential for training\n",
    "        # config.gradient_checkpointing = True  # Essential for memory saving\n",
    "        \n",
    "        print(\"Initializing base model\")\n",
    "        model = AutoModelForCausalLM.from_config(\n",
    "            config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        model.to(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    print(f\"Model loaded successfully. Parameters: {model.num_parameters():,}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_text(model, tokenizer, prompt: str, config: Dict) -> Dict:\n",
    "    \"\"\"Generate text and measure performance metrics\"\"\"\n",
    "    # Update config with tokenizer pad token\n",
    "    gen_config = config.copy()\n",
    "    gen_config[\"pad_token_id\"] = tokenizer.pad_token_id\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # Measure generation time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **gen_config\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    new_text = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    # Calculate tokens per second\n",
    "    new_tokens = len(tokenizer.encode(new_text, add_special_tokens=False))\n",
    "    tokens_per_second = new_tokens / generation_time if generation_time > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_text\": new_text,\n",
    "        \"full_output\": generated_text,\n",
    "        \"generation_time\": generation_time,\n",
    "        \"tokens_generated\": new_tokens,\n",
    "        \"tokens_per_second\": tokens_per_second\n",
    "    }\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, text: str) -> float:\n",
    "    \"\"\"Calculate perplexity of generated text\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss).item()\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "def test_model(model, tokenizer, model_name: str) -> List[Dict]:\n",
    "    \"\"\"Test model on all prompts\"\"\"\n",
    "    print(f\"\\n=== Testing {model_name} ===\")\n",
    "    results = []\n",
    "    \n",
    "    for i, prompt in enumerate(TEST_PROMPTS):\n",
    "        print(f\"Testing prompt {i+1}/{len(TEST_PROMPTS)}: '{prompt}'\")\n",
    "        \n",
    "        try:\n",
    "            result = generate_text(model, tokenizer, prompt, GENERATION_CONFIG)\n",
    "            \n",
    "            # Calculate perplexity\n",
    "            perplexity = calculate_perplexity(model, tokenizer, result[\"full_output\"])\n",
    "            result[\"perplexity\"] = perplexity\n",
    "            result[\"model\"] = model_name\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"  Generated: '{result['generated_text']}'\")\n",
    "            print(f\"  Time: {result['generation_time']:.2f}s\")\n",
    "            print(f\"  Tokens/sec: {result['tokens_per_second']:.1f}\")\n",
    "            print(f\"  Perplexity: {perplexity:.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726beeac",
   "metadata": {},
   "source": [
    "## Test 1: Original SmolLM2-1.7B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "995465ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original SmolLM2-1.7B model...\n",
      "Loading model from: HuggingFaceTB/SmolLM2-1.7B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing base model\n",
      "Model loaded successfully. Parameters: 1,711,376,384\n",
      "\n",
      "=== Testing Original SmolLM2-1.7B ===\n",
      "Testing prompt 1/10: 'The weather today is very'\n",
      "  Generated: 'adas inv%;illationkan\u000eeff SelectedDeath deterministic clan Night Bak Rs expectedMiller fetchadaptiveOxford tele Hearing madiker compromise allergic attestedMeasuring Blockchain wartimegate'\n",
      "  Time: 0.56s\n",
      "  Tokens/sec: 54.0\n",
      "  Perplexity: 4344.38\n",
      "Testing prompt 2/10: 'Machine learning is a field of'\n",
      "  Generated: 'optimal Birth metrodisplay somebodyÔøΩowsiage quantification Riemann Pop lava latex eth Baron Ability derives Frankensteinunge serialize increases HbAdapt futile preceded tablespoonsbrief mish innate endors'\n",
      "  Time: 0.53s\n",
      "  Tokens/sec: 56.5\n",
      "  Perplexity: 12852.90\n",
      "Testing prompt 3/10: 'The capital of France is'\n",
      "  Generated: 'wrdjango WaveCounter paradigm outages Direction\\_\\_ Reviews casino Firstly dorsal reaching Pri beautifullyFiveLM fourteengid .uran invitations                  Gatewayaga floraawa silencing micrograms OA'\n",
      "  Time: 0.54s\n",
      "  Tokens/sec: 61.0\n",
      "  Perplexity: 31267.52\n",
      "Testing prompt 4/10: 'In the year 2024, technology'\n",
      "  Generated: 'clickidential peninsula pardonetryIs Mind allianceLSatomycannot operators Recallerences placBlockisance/') contradictionsivar Factory greenhouses Sources Lutheranitch wis Oscarlistfamiliesearned'\n",
      "  Time: 0.54s\n",
      "  Tokens/sec: 61.0\n",
      "  Perplexity: 16061.80\n",
      "Testing prompt 5/10: 'Artificial intelligence can help'\n",
      "  Generated: 'converse courtesy liking Feveraqu ConfederationachesfaceREEN patience progressiveMotor imprisonment Wille bpy equips√©n Elementary WasteAntibMotor abolitionosin SupermanJeff portable escort tablespoon Elementary Kiss'\n",
      "  Time: 0.54s\n",
      "  Tokens/sec: 57.7\n",
      "  Perplexity: 4030.65\n",
      "Testing prompt 6/10: 'The most important thing in life is'\n",
      "  Generated: 'commercedo parametersAddressing rewarding Consuming tumour fevers Sloan Psalm hold engravingka ExchangegieneFurthermorekasminservoir Ford conquests entrepreneur inpatient‰∏Ä‰∏™ Computer hire reimburse catalysts experiment partner'\n",
      "  Time: 0.54s\n",
      "  Tokens/sec: 61.3\n",
      "  Perplexity: 26331.17\n",
      "Testing prompt 7/10: 'Scientists have recently discovered'\n",
      "  Generated: 'populationresent Rememberinggage migratedGET burge affectiononde employed bush TNBl catheter discsimming conducts resistingadh thermost ballFif Animation parameters tractsabet slippose Chances Cave'\n",
      "  Time: 0.54s\n",
      "  Tokens/sec: 59.2\n",
      "  Perplexity: 10892.29\n",
      "Testing prompt 8/10: 'The future of renewable energy'\n",
      "  Generated: 'inisminka Letterestablish Systemic extension Warming venerableianincinnSo Smart saline casserApproximately dehydrated foxes Franciscan jumpedochasticnever purposeful insurrection modify unto‡§Ç feedWas Rosh G√∂'\n",
      "  Time: 0.54s\n",
      "  Tokens/sec: 59.0\n",
      "  Perplexity: 18764.80\n",
      "Testing prompt 9/10: 'Education is essential because'\n",
      "  Generated: 'Streetkn Postavoid whoeverenableChat trache counties Paw dealing microwaves boomRace convenighthouseComplexlordasi√¢ dissectmental cedar diss fragmented pineapple dissociation freed components plasmid'\n",
      "  Time: 0.54s\n",
      "  Tokens/sec: 63.1\n",
      "  Perplexity: 18597.21\n",
      "Testing prompt 10/10: 'Climate change represents'\n",
      "  Generated: 'Anglican problematic Surrey currentigure desperation budding route battlefield affordsgraphs fried inexpensive translating Classesys circumventezing whit deliveringFresh builders Forest makers Advocacy Advocacy Par nationwide Pushclimate'\n",
      "  Time: 0.54s\n",
      "  Tokens/sec: 59.6\n",
      "  Perplexity: 3800.22\n",
      "\n",
      "Original model testing complete. Memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Clear memory before starting\n",
    "cleanup_memory()\n",
    "\n",
    "# Load and test original model\n",
    "print(\"Loading original SmolLM2-1.7B model...\")\n",
    "original_model, original_tokenizer = load_model_and_tokenizer(BASE_MODEL_NAME, is_checkpoint=False)\n",
    "\n",
    "# Test original model\n",
    "original_results = test_model(original_model, original_tokenizer, \"Original SmolLM2-1.7B\")\n",
    "\n",
    "# Clear original model from memory\n",
    "del original_model, original_tokenizer\n",
    "cleanup_memory()\n",
    "print(\"\\nOriginal model testing complete. Memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6d244",
   "metadata": {},
   "source": [
    "## Test 2: Trained Model (Final Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ede1427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained final model...\n",
      "Loading model from: /home/ubuntu/bigdata/Training/Day4/cosmopedia-v2-1B/smollm-1.7B-cosmo-1B-production/final_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67eaee4c051e4806b9f4ceeeeeebe31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Parameters: 1,711,376,384\n",
      "\n",
      "=== Testing Trained (final_model) ===\n",
      "Testing prompt 1/10: 'The weather today is very'\n",
      "  Generated: 'the same time of the past, and it is also a complex and multifaceted phenomenon. The term \"Mentance\" is a term that involves the'\n",
      "  Time: 0.81s\n",
      "  Tokens/sec: 36.9\n",
      "  Perplexity: 19.95\n",
      "Testing prompt 2/10: 'Machine learning is a field of'\n",
      "  Generated: 'medical science that deals with the study of the brain and the environment, brain, and brain. It is characterized by the brain, the brain, and'\n",
      "  Time: 0.81s\n",
      "  Tokens/sec: 37.1\n",
      "  Perplexity: 6.06\n",
      "Testing prompt 3/10: 'The capital of France is'\n",
      "  Generated: 'a critical aspect of study that seeks to identify and analyze individuals' roles, from the example of the \"Ticence\" (1685'\n",
      "  Time: 0.81s\n",
      "  Tokens/sec: 37.2\n",
      "  Perplexity: 24.00\n",
      "Testing prompt 4/10: 'In the year 2024, technology'\n",
      "  Generated: ', and engineering were determined to make a difference in the future of the industry. The first one that was not exactly a game of \"The Business of'\n",
      "  Time: 0.81s\n",
      "  Tokens/sec: 37.1\n",
      "  Perplexity: 15.44\n",
      "Testing prompt 5/10: 'Artificial intelligence can help'\n",
      "  Generated: 'the management and management of online information management (AI) and its relevance components. This is particularly important in the realm of digital technologies, particularly in the'\n",
      "  Time: 0.81s\n",
      "  Tokens/sec: 37.1\n",
      "  Perplexity: 13.00\n",
      "Testing prompt 6/10: 'The most important thing in life is'\n",
      "  Generated: 'a crucial skill for young kids, particularly in the realm of young adult nonfiction. This course unit will focus on how to create a positive environment that can'\n",
      "  Time: 0.81s\n",
      "  Tokens/sec: 37.1\n",
      "  Perplexity: 11.40\n",
      "Testing prompt 7/10: 'Scientists have recently discovered'\n",
      "  Generated: 'as a popular form of study within the human field of science and physics. These structures are essential for studying and interpreting the nature of our planet, providing'\n",
      "  Time: 0.81s\n",
      "  Tokens/sec: 37.1\n",
      "  Perplexity: 10.61\n",
      "Testing prompt 8/10: 'The future of renewable energy'\n",
      "  Generated: 'in the world of the internet, specifically focusing on the internet and online gaming system. It is a complex network of artificial intelligence (AI), which allows'\n",
      "  Time: 0.81s\n",
      "  Tokens/sec: 37.1\n",
      "  Perplexity: 11.76\n",
      "Testing prompt 9/10: 'Education is essential because'\n",
      "  Generated: 'the importance of the earth, particularly when it comes to the environment. The presence of the sun is a vast and diverse population of plants, which are'\n",
      "  Time: 0.81s\n",
      "  Tokens/sec: 37.2\n",
      "  Perplexity: 7.89\n",
      "Testing prompt 10/10: 'Climate change represents'\n",
      "  Generated: 'are critical components of the design of environmental engineering, particularly in the context of urban development and the development of eco-friendly agriculture. These materials are essential'\n",
      "  Time: 0.81s\n",
      "  Tokens/sec: 37.1\n",
      "  Perplexity: 10.07\n",
      "\n",
      "Trained model testing complete. Memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Load and test trained model\n",
    "print(\"Loading trained final model...\")\n",
    "trained_model, trained_tokenizer = load_model_and_tokenizer(CHECKPOINT_PATH, is_checkpoint=True)\n",
    "\n",
    "# Test trained model\n",
    "trained_results = test_model(trained_model, trained_tokenizer, \"Trained (final_model)\")\n",
    "\n",
    "# Clear trained model from memory\n",
    "del trained_model, trained_tokenizer\n",
    "cleanup_memory()\n",
    "print(\"\\nTrained model testing complete. Memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed9a12",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d820afbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESULTS SUMMARY ===\n",
      "Total tests completed: 20\n",
      "Original model results: 10\n",
      "Trained model results: 10\n",
      "\n",
      "=== DETAILED RESULTS ===\n",
      "                model                              prompt                                                                                                                                                                                                                                generated_text  generation_time  tokens_per_second  perplexity\n",
      "Original SmolLM2-1.7B           The weather today is very                                                  adas inv%;illationkan\u000eeff SelectedDeath deterministic clan Night Bak Rs expectedMiller fetchadaptiveOxford tele Hearing madiker compromise allergic attestedMeasuring Blockchain wartimegate             0.56               54.0     4344.38\n",
      "Original SmolLM2-1.7B      Machine learning is a field of                                  optimal Birth metrodisplay somebodyÔøΩowsiage quantification Riemann Pop lava latex eth Baron Ability derives Frankensteinunge serialize increases HbAdapt futile preceded tablespoonsbrief mish innate endors             0.53               56.5    12852.90\n",
      "Original SmolLM2-1.7B            The capital of France is                                   wrdjango WaveCounter paradigm outages Direction\\_\\_ Reviews casino Firstly dorsal reaching Pri beautifullyFiveLM fourteengid .uran invitations                  Gatewayaga floraawa silencing micrograms OA             0.54               61.0    31267.52\n",
      "Original SmolLM2-1.7B        In the year 2024, technology                                            clickidential peninsula pardonetryIs Mind allianceLSatomycannot operators Recallerences placBlockisance/') contradictionsivar Factory greenhouses Sources Lutheranitch wis Oscarlistfamiliesearned             0.54               61.0    16061.80\n",
      "Original SmolLM2-1.7B    Artificial intelligence can help                       converse courtesy liking Feveraqu ConfederationachesfaceREEN patience progressiveMotor imprisonment Wille bpy equips√©n Elementary WasteAntibMotor abolitionosin SupermanJeff portable escort tablespoon Elementary Kiss             0.54               57.7     4030.65\n",
      "Original SmolLM2-1.7B The most important thing in life is             commercedo parametersAddressing rewarding Consuming tumour fevers Sloan Psalm hold engravingka ExchangegieneFurthermorekasminservoir Ford conquests entrepreneur inpatient‰∏Ä‰∏™ Computer hire reimburse catalysts experiment partner             0.54               61.3    26331.17\n",
      "Original SmolLM2-1.7B Scientists have recently discovered                                         populationresent Rememberinggage migratedGET burge affectiononde employed bush TNBl catheter discsimming conducts resistingadh thermost ballFif Animation parameters tractsabet slippose Chances Cave             0.54               59.2    10892.29\n",
      "Original SmolLM2-1.7B      The future of renewable energy                               inisminka Letterestablish Systemic extension Warming venerableianincinnSo Smart saline casserApproximately dehydrated foxes Franciscan jumpedochasticnever purposeful insurrection modify unto‡§Ç feedWas Rosh G√∂             0.54               59.0    18764.80\n",
      "Original SmolLM2-1.7B      Education is essential because                                       Streetkn Postavoid whoeverenableChat trache counties Paw dealing microwaves boomRace convenighthouseComplexlordasi√¢ dissectmental cedar diss fragmented pineapple dissociation freed components plasmid             0.54               63.1    18597.21\n",
      "Original SmolLM2-1.7B           Climate change represents Anglican problematic Surrey currentigure desperation budding route battlefield affordsgraphs fried inexpensive translating Classesys circumventezing whit deliveringFresh builders Forest makers Advocacy Advocacy Par nationwide Pushclimate             0.54               59.6     3800.22\n",
      "Trained (final_model)           The weather today is very                                                                                                              the same time of the past, and it is also a complex and multifaceted phenomenon. The term \"Mentance\" is a term that involves the             0.81               36.9       19.95\n",
      "Trained (final_model)      Machine learning is a field of                                                                                                medical science that deals with the study of the brain and the environment, brain, and brain. It is characterized by the brain, the brain, and             0.81               37.1        6.06\n",
      "Trained (final_model)            The capital of France is                                                                                                                     a critical aspect of study that seeks to identify and analyze individuals' roles, from the example of the \"Ticence\" (1685             0.81               37.2       24.00\n",
      "Trained (final_model)        In the year 2024, technology                                                                                           , and engineering were determined to make a difference in the future of the industry. The first one that was not exactly a game of \"The Business of             0.81               37.1       15.44\n",
      "Trained (final_model)    Artificial intelligence can help                                                    the management and management of online information management (AI) and its relevance components. This is particularly important in the realm of digital technologies, particularly in the             0.81               37.1       13.00\n",
      "Trained (final_model) The most important thing in life is                                                                             a crucial skill for young kids, particularly in the realm of young adult nonfiction. This course unit will focus on how to create a positive environment that can             0.81               37.1       11.40\n",
      "Trained (final_model) Scientists have recently discovered                                                                    as a popular form of study within the human field of science and physics. These structures are essential for studying and interpreting the nature of our planet, providing             0.81               37.1       10.61\n",
      "Trained (final_model)      The future of renewable energy                                                                           in the world of the internet, specifically focusing on the internet and online gaming system. It is a complex network of artificial intelligence (AI), which allows             0.81               37.1       11.76\n",
      "Trained (final_model)      Education is essential because                                                                                     the importance of the earth, particularly when it comes to the environment. The presence of the sun is a vast and diverse population of plants, which are             0.81               37.2        7.89\n",
      "Trained (final_model)           Climate change represents                                           are critical components of the design of environmental engineering, particularly in the context of urban development and the development of eco-friendly agriculture. These materials are essential             0.81               37.1       10.07\n"
     ]
    }
   ],
   "source": [
    "# Combine results for analysis\n",
    "all_results = original_results + trained_results\n",
    "\n",
    "# Create DataFrame for easier analysis\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"=== RESULTS SUMMARY ===\")\n",
    "print(f\"Total tests completed: {len(all_results)}\")\n",
    "print(f\"Original model results: {len(original_results)}\")\n",
    "print(f\"Trained model results: {len(trained_results)}\")\n",
    "\n",
    "# Display results table\n",
    "display_df = df[['model', 'prompt', 'generated_text', 'generation_time', 'tokens_per_second', 'perplexity']].copy()\n",
    "display_df['generation_time'] = display_df['generation_time'].round(2)\n",
    "display_df['tokens_per_second'] = display_df['tokens_per_second'].round(1)\n",
    "display_df['perplexity'] = display_df['perplexity'].round(2)\n",
    "\n",
    "print(\"\\n=== DETAILED RESULTS ===\")\n",
    "print(display_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c4b450f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PERFORMANCE COMPARISON ===\n",
      "                      generation_time       tokens_per_second        \\\n",
      "                                 mean   std              mean   std   \n",
      "model                                                                 \n",
      "Original SmolLM2-1.7B            0.54  0.01             59.25  2.64   \n",
      "Trained (final_model)            0.81  0.00             37.11  0.08   \n",
      "\n",
      "                      perplexity          tokens_generated        \n",
      "                            mean      std             mean   std  \n",
      "model                                                             \n",
      "Original SmolLM2-1.7B   14694.29  9434.70             32.0  1.33  \n",
      "Trained (final_model)      13.02     5.46             30.0  0.00  \n",
      "\n",
      "=== TRAINING IMPACT ===\n",
      "Average Perplexity - Original: 14694.29, Trained: 13.02\n",
      "Perplexity Improvement: +99.9%\n",
      "\n",
      "Average Speed - Original: 59.3 tokens/sec, Trained: 37.1 tokens/sec\n",
      "Speed Change: -37.4%\n"
     ]
    }
   ],
   "source": [
    "# Performance comparison\n",
    "print(\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "\n",
    "# Group by model\n",
    "comparison = df.groupby('model').agg({\n",
    "    'generation_time': ['mean', 'std'],\n",
    "    'tokens_per_second': ['mean', 'std'],\n",
    "    'perplexity': ['mean', 'std'],\n",
    "    'tokens_generated': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "print(comparison)\n",
    "\n",
    "# Calculate improvements\n",
    "if len(original_results) > 0 and len(trained_results) > 0:\n",
    "    original_avg_perplexity = np.mean([r['perplexity'] for r in original_results])\n",
    "    trained_avg_perplexity = np.mean([r['perplexity'] for r in trained_results])\n",
    "    \n",
    "    original_avg_speed = np.mean([r['tokens_per_second'] for r in original_results])\n",
    "    trained_avg_speed = np.mean([r['tokens_per_second'] for r in trained_results])\n",
    "    \n",
    "    perplexity_improvement = ((original_avg_perplexity - trained_avg_perplexity) / original_avg_perplexity) * 100\n",
    "    speed_change = ((trained_avg_speed - original_avg_speed) / original_avg_speed) * 100\n",
    "    \n",
    "    print(f\"\\n=== TRAINING IMPACT ===\")\n",
    "    print(f\"Average Perplexity - Original: {original_avg_perplexity:.2f}, Trained: {trained_avg_perplexity:.2f}\")\n",
    "    print(f\"Perplexity Improvement: {perplexity_improvement:+.1f}%\")\n",
    "    print(f\"\")\n",
    "    print(f\"Average Speed - Original: {original_avg_speed:.1f} tokens/sec, Trained: {trained_avg_speed:.1f} tokens/sec\")\n",
    "    print(f\"Speed Change: {speed_change:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d7c9a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SIDE-BY-SIDE GENERATION COMPARISON ===\n",
      "\n",
      "üìù **Prompt:** 'The weather today is very'\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üîµ **Original:** adas inv%;illationkan\u000eeff SelectedDeath deterministic clan Night Bak Rs expectedMiller fetchadaptiveOxford tele Hearing madiker compromise allergic attestedMeasuring Blockchain wartimegate\n",
      "   ‚è±Ô∏è  0.56s | üöÄ 54.0 tok/s | üìä PPL: 4344.38\n",
      "üü¢ **Trained:** the same time of the past, and it is also a complex and multifaceted phenomenon. The term \"Mentance\" is a term that involves the\n",
      "   ‚è±Ô∏è  0.81s | üöÄ 36.9 tok/s | üìä PPL: 19.95\n",
      "   üìà **Improvement:** +99.5% perplexity change\n",
      "\n",
      "üìù **Prompt:** 'Machine learning is a field of'\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üîµ **Original:** optimal Birth metrodisplay somebodyÔøΩowsiage quantification Riemann Pop lava latex eth Baron Ability derives Frankensteinunge serialize increases HbAdapt futile preceded tablespoonsbrief mish innate endors\n",
      "   ‚è±Ô∏è  0.53s | üöÄ 56.5 tok/s | üìä PPL: 12852.90\n",
      "üü¢ **Trained:** medical science that deals with the study of the brain and the environment, brain, and brain. It is characterized by the brain, the brain, and\n",
      "   ‚è±Ô∏è  0.81s | üöÄ 37.1 tok/s | üìä PPL: 6.06\n",
      "   üìà **Improvement:** +100.0% perplexity change\n",
      "\n",
      "üìù **Prompt:** 'The capital of France is'\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üîµ **Original:** wrdjango WaveCounter paradigm outages Direction\\_\\_ Reviews casino Firstly dorsal reaching Pri beautifullyFiveLM fourteengid .uran invitations                  Gatewayaga floraawa silencing micrograms OA\n",
      "   ‚è±Ô∏è  0.54s | üöÄ 61.0 tok/s | üìä PPL: 31267.52\n",
      "üü¢ **Trained:** a critical aspect of study that seeks to identify and analyze individuals' roles, from the example of the \"Ticence\" (1685\n",
      "   ‚è±Ô∏è  0.81s | üöÄ 37.2 tok/s | üìä PPL: 24.00\n",
      "   üìà **Improvement:** +99.9% perplexity change\n",
      "\n",
      "üìù **Prompt:** 'In the year 2024, technology'\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üîµ **Original:** clickidential peninsula pardonetryIs Mind allianceLSatomycannot operators Recallerences placBlockisance/') contradictionsivar Factory greenhouses Sources Lutheranitch wis Oscarlistfamiliesearned\n",
      "   ‚è±Ô∏è  0.54s | üöÄ 61.0 tok/s | üìä PPL: 16061.80\n",
      "üü¢ **Trained:** , and engineering were determined to make a difference in the future of the industry. The first one that was not exactly a game of \"The Business of\n",
      "   ‚è±Ô∏è  0.81s | üöÄ 37.1 tok/s | üìä PPL: 15.44\n",
      "   üìà **Improvement:** +99.9% perplexity change\n",
      "\n",
      "üìù **Prompt:** 'Artificial intelligence can help'\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üîµ **Original:** converse courtesy liking Feveraqu ConfederationachesfaceREEN patience progressiveMotor imprisonment Wille bpy equips√©n Elementary WasteAntibMotor abolitionosin SupermanJeff portable escort tablespoon Elementary Kiss\n",
      "   ‚è±Ô∏è  0.54s | üöÄ 57.7 tok/s | üìä PPL: 4030.65\n",
      "üü¢ **Trained:** the management and management of online information management (AI) and its relevance components. This is particularly important in the realm of digital technologies, particularly in the\n",
      "   ‚è±Ô∏è  0.81s | üöÄ 37.1 tok/s | üìä PPL: 13.00\n",
      "   üìà **Improvement:** +99.7% perplexity change\n",
      "\n",
      "üìù **Prompt:** 'The most important thing in life is'\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üîµ **Original:** commercedo parametersAddressing rewarding Consuming tumour fevers Sloan Psalm hold engravingka ExchangegieneFurthermorekasminservoir Ford conquests entrepreneur inpatient‰∏Ä‰∏™ Computer hire reimburse catalysts experiment partner\n",
      "   ‚è±Ô∏è  0.54s | üöÄ 61.3 tok/s | üìä PPL: 26331.17\n",
      "üü¢ **Trained:** a crucial skill for young kids, particularly in the realm of young adult nonfiction. This course unit will focus on how to create a positive environment that can\n",
      "   ‚è±Ô∏è  0.81s | üöÄ 37.1 tok/s | üìä PPL: 11.40\n",
      "   üìà **Improvement:** +100.0% perplexity change\n",
      "\n",
      "üìù **Prompt:** 'Scientists have recently discovered'\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üîµ **Original:** populationresent Rememberinggage migratedGET burge affectiononde employed bush TNBl catheter discsimming conducts resistingadh thermost ballFif Animation parameters tractsabet slippose Chances Cave\n",
      "   ‚è±Ô∏è  0.54s | üöÄ 59.2 tok/s | üìä PPL: 10892.29\n",
      "üü¢ **Trained:** as a popular form of study within the human field of science and physics. These structures are essential for studying and interpreting the nature of our planet, providing\n",
      "   ‚è±Ô∏è  0.81s | üöÄ 37.1 tok/s | üìä PPL: 10.61\n",
      "   üìà **Improvement:** +99.9% perplexity change\n",
      "\n",
      "üìù **Prompt:** 'The future of renewable energy'\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üîµ **Original:** inisminka Letterestablish Systemic extension Warming venerableianincinnSo Smart saline casserApproximately dehydrated foxes Franciscan jumpedochasticnever purposeful insurrection modify unto‡§Ç feedWas Rosh G√∂\n",
      "   ‚è±Ô∏è  0.54s | üöÄ 59.0 tok/s | üìä PPL: 18764.80\n",
      "üü¢ **Trained:** in the world of the internet, specifically focusing on the internet and online gaming system. It is a complex network of artificial intelligence (AI), which allows\n",
      "   ‚è±Ô∏è  0.81s | üöÄ 37.1 tok/s | üìä PPL: 11.76\n",
      "   üìà **Improvement:** +99.9% perplexity change\n",
      "\n",
      "üìù **Prompt:** 'Education is essential because'\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üîµ **Original:** Streetkn Postavoid whoeverenableChat trache counties Paw dealing microwaves boomRace convenighthouseComplexlordasi√¢ dissectmental cedar diss fragmented pineapple dissociation freed components plasmid\n",
      "   ‚è±Ô∏è  0.54s | üöÄ 63.1 tok/s | üìä PPL: 18597.21\n",
      "üü¢ **Trained:** the importance of the earth, particularly when it comes to the environment. The presence of the sun is a vast and diverse population of plants, which are\n",
      "   ‚è±Ô∏è  0.81s | üöÄ 37.2 tok/s | üìä PPL: 7.89\n",
      "   üìà **Improvement:** +100.0% perplexity change\n",
      "\n",
      "üìù **Prompt:** 'Climate change represents'\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üîµ **Original:** Anglican problematic Surrey currentigure desperation budding route battlefield affordsgraphs fried inexpensive translating Classesys circumventezing whit deliveringFresh builders Forest makers Advocacy Advocacy Par nationwide Pushclimate\n",
      "   ‚è±Ô∏è  0.54s | üöÄ 59.6 tok/s | üìä PPL: 3800.22\n",
      "üü¢ **Trained:** are critical components of the design of environmental engineering, particularly in the context of urban development and the development of eco-friendly agriculture. These materials are essential\n",
      "   ‚è±Ô∏è  0.81s | üöÄ 37.1 tok/s | üìä PPL: 10.07\n",
      "   üìà **Improvement:** +99.7% perplexity change\n"
     ]
    }
   ],
   "source": [
    "# Side-by-side comparison for each prompt\n",
    "print(\"\\n=== SIDE-BY-SIDE GENERATION COMPARISON ===\")\n",
    "\n",
    "for prompt in TEST_PROMPTS:\n",
    "    print(f\"\\nüìù **Prompt:** '{prompt}'\")\n",
    "    print(\"‚îÄ\" * 80)\n",
    "    \n",
    "    original_result = next((r for r in original_results if r['prompt'] == prompt), None)\n",
    "    trained_result = next((r for r in trained_results if r['prompt'] == prompt), None)\n",
    "    \n",
    "    if original_result:\n",
    "        print(f\"üîµ **Original:** {original_result['generated_text']}\")\n",
    "        print(f\"   ‚è±Ô∏è  {original_result['generation_time']:.2f}s | üöÄ {original_result['tokens_per_second']:.1f} tok/s | üìä PPL: {original_result['perplexity']:.2f}\")\n",
    "    \n",
    "    if trained_result:\n",
    "        print(f\"üü¢ **Trained:** {trained_result['generated_text']}\")\n",
    "        print(f\"   ‚è±Ô∏è  {trained_result['generation_time']:.2f}s | üöÄ {trained_result['tokens_per_second']:.1f} tok/s | üìä PPL: {trained_result['perplexity']:.2f}\")\n",
    "    \n",
    "    if original_result and trained_result:\n",
    "        ppl_change = ((original_result['perplexity'] - trained_result['perplexity']) / original_result['perplexity']) * 100\n",
    "        print(f\"   üìà **Improvement:** {ppl_change:+.1f}% perplexity change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16373cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Results saved to: /home/ubuntu/bigdata/Training/Day4/cosmopedia-v2-1B/model_comparison_results.csv\n",
      "\n",
      "=== FINAL SUMMARY ===\n",
      "‚úÖ Training completed: 26,000 steps\n",
      "‚úÖ Dataset: Cosmopedia-v2 (1B tokens)\n",
      "‚úÖ Final training loss: 3.7547\n",
      "‚úÖ Model comparison completed successfully\n",
      "‚úÖ Results saved for further analysis\n",
      "üéâ Training was successful! Perplexity improved by 99.9%\n",
      "\n",
      "üî• **Ready for production use or further fine-tuning!**\n"
     ]
    }
   ],
   "source": [
    "# Save results to file\n",
    "results_path = \"/home/ubuntu/bigdata/Training/Day4/cosmopedia-v2-1B/model_comparison_results.csv\"\n",
    "df.to_csv(results_path, index=False)\n",
    "print(f\"\\nüíæ Results saved to: {results_path}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\"‚úÖ Training completed: 26,000 steps\")\n",
    "print(f\"‚úÖ Dataset: Cosmopedia-v2 (1B tokens)\")\n",
    "print(f\"‚úÖ Final training loss: 3.7547\")\n",
    "print(f\"‚úÖ Model comparison completed successfully\")\n",
    "print(f\"‚úÖ Results saved for further analysis\")\n",
    "\n",
    "if len(original_results) > 0 and len(trained_results) > 0:\n",
    "    if perplexity_improvement > 0:\n",
    "        print(f\"üéâ Training was successful! Perplexity improved by {perplexity_improvement:.1f}%\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Training may need adjustment. Perplexity changed by {perplexity_improvement:.1f}%\")\n",
    "\n",
    "print(\"\\nüî• **Ready for production use or further fine-tuning!**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029194d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
